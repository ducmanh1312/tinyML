{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icnlab/anaconda3/envs/aiot/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# include\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from pathlib import Path\n",
    "from pytorch_metric_learning import losses\n",
    "\n",
    "from PDDD.Codes.modelpy.visual_model.ResNet_50_101_152 import ResNet152\n",
    "from PDDD.Codes.modelpy.visual_model.ViT_L import VisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config \n",
    "BATCH_SIZE = 20\n",
    "# NUM_CLASSES = 27\n",
    "BASE_LR = 0.001\n",
    "WEIGHT_DECAY = 0.05 #  L2 regularization \n",
    "WARMUP_EPOCHS = 2 # Trong 2 epoch đầu, learning rate sẽ được tăng dần từ nhỏ đến BASE_LR\n",
    "EPOCHS = 100\n",
    "LAYER_DECAY = 0.8 # (layer) gần đầu vào có learning rate thấp hơn, và các tầng gần đầu ra (cuối mô hình) có learning rate cao hơn\n",
    "ACCUM_GRAD_STEPS = 1 # Gradient accumulation: để mô phỏng batch size lớn hơn.\n",
    "\n",
    "checkpoint_dir = \"checkpoints/no_projection_head\"\n",
    "DATA_DIR = \"/media/icnlab/Data/Manh/tinyML/FieldPlant-11/cropped\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataModule\n",
    "# ----------------------------\n",
    "class FilteredImageFolder(ImageFolder):\n",
    "    def __init__(self, root, included_classes, **kwargs):\n",
    "        super().__init__(root, **kwargs)\n",
    "        # Lưu lại các chỉ số class cần giữ\n",
    "        included_indices = [self.class_to_idx[cls] for cls in included_classes]\n",
    "        self.samples = [s for s in self.samples if s[1] in included_indices]\n",
    "        self.targets = [s[1] for s in self.samples]\n",
    "\n",
    "class Dataset(pl.LightningDataModule):\n",
    "    # init dataset, split, transform, dataloader\n",
    "    def __init__(self, data_dir, batch_size, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "        super().__init__()\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Define separate transforms for training (with augmentation) and evaluation\n",
    "        self.train_transform = T.Compose([\n",
    "            T.Resize(256),\n",
    "            T.CenterCrop(224),\n",
    "            T.RandomHorizontalFlip(p=0.5),  # Randomly flip images horizontally\n",
    "            T.RandomRotation(degrees=15),    # Randomly rotate images by up to 15 degrees\n",
    "            T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),  # Random color adjustments\n",
    "            T.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random translations\n",
    "            T.RandomPerspective(distortion_scale=0.2, p=0.5),  # Random perspective changes\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet normalization\n",
    "        ])\n",
    "        \n",
    "        # Transform for validation and testing (no augmentation)\n",
    "        self.eval_transform = T.Compose([\n",
    "            T.Resize(256),\n",
    "            T.CenterCrop(224),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.416, 0.468, 0.355],   # normalize\n",
    "                                std=[0.210, 0.206, 0.213])\n",
    "        ])  # in PDDD paper\n",
    "        \n",
    "        # Ensure ratios sum to 1\n",
    "        assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-5, \"Ratios must sum to 1\"\n",
    "        self.train_ratio = train_ratio\n",
    "        self.val_ratio = val_ratio\n",
    "        self.test_ratio = test_ratio\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Load the full dataset with eval transform initially\n",
    "        self.full_dataset = ImageFolder(self.data_dir, transform=self.eval_transform)\n",
    "\n",
    "        # bug\n",
    "        # included = ['Tomato Brown Spots', 'Tomato blight leaf', 'Tomato healthy', 'Tomato leaf yellow virus']\n",
    "        # self.full_dataset = FilteredImageFolder(self.data_dir, included, transform=self.eval_transform)\n",
    "        \n",
    "    # Calculate split sizes\n",
    "        dataset_size = len(self.full_dataset)\n",
    "        train_size = int(dataset_size * self.train_ratio)\n",
    "        val_size = int(dataset_size * self.val_ratio)\n",
    "        test_size = dataset_size - train_size - val_size\n",
    "        \n",
    "        # Split the dataset\n",
    "        from torch.utils.data import random_split\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(\n",
    "            self.full_dataset,\n",
    "            [train_size, val_size, test_size],\n",
    "            generator=torch.Generator().manual_seed(42)  # For reproducibility\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=8)\n",
    "        \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer-wise LR decay helper\n",
    "# ----------------------------\n",
    "def get_layer_decay_param_groups(model, base_lr, weight_decay, layer_decay):\n",
    "    param_groups = []\n",
    "    layers = list(model.named_parameters())\n",
    "    num_layers = len(layers)\n",
    "\n",
    "    for i, (name, param) in enumerate(layers):\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        lr = base_lr * (layer_decay ** (num_layers - i - 1))\n",
    "        param_groups.append({\n",
    "            \"params\": [param],\n",
    "            \"lr\": lr,\n",
    "            \"weight_decay\": weight_decay if param.ndim >= 2 else 0.0\n",
    "        })\n",
    "    return param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning Module\n",
    "# ----------------------------\n",
    "class EVA02Lightning(pl.LightningModule):\n",
    "    def __init__(self, embedding_dim=2048, projection_dim=128):\n",
    "        super().__init__()\n",
    "        # Load the base model\n",
    "        model = ResNet152()\n",
    "        model_path = \"/media/icnlab/Data/Manh/tinyML/PDDD/model/ResNet152.std\"\n",
    "        model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if name.startswith(('conv1', 'layer1', 'layer2', 'layer3')):\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "        \n",
    "    # Add a projection head for contrastive learning\n",
    "        self.backbone = nn.Sequential(*list(model.children())[:-1])  # Remove final classification layer\n",
    "        \n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, projection_dim)\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            self.backbone = torch.compile(self.backbone)\n",
    "        except Exception:\n",
    "            pass  # torch.compile requires PyTorch 2+\n",
    "\n",
    "        # Define contrastive loss\n",
    "        self.loss_fn = losses.SupConLoss(temperature=0.07)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get embeddings from backbone\n",
    "        features = self.backbone(x)\n",
    "        # Handle potential dimension issues\n",
    "        if len(features.shape) > 2:\n",
    "            features = features.squeeze()\n",
    "        # Get projected embeddings\n",
    "        projections = self.projection_head(features)\n",
    "        # Normalize projections for contrastive loss\n",
    "        normalized_projections = nn.functional.normalize(projections, dim=1)\n",
    "        return features, normalized_projections\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        \n",
    "        # Get embeddings and normalized projections\n",
    "        _, normalized_projections = self(x)\n",
    "        \n",
    "        # Calculate contrastive loss\n",
    "        loss = self.loss_fn(normalized_projections, y)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        \n",
    "        # For validation, we'll just log the contrastive loss\n",
    "        _, normalized_projections = self(x)\n",
    "        val_loss = self.loss_fn(normalized_projections, y)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True)\n",
    "        \n",
    "        # Since we don't have a classifier, we can't calculate accuracy directly\n",
    "        # We could implement a nearest-neighbor evaluation here if needed\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Get parameters with layer-wise decay for backbone\n",
    "        param_groups = get_layer_decay_param_groups(self.backbone, BASE_LR, WEIGHT_DECAY, LAYER_DECAY)\n",
    "        \n",
    "        # Add projection head parameters\n",
    "        param_groups.append({\n",
    "            \"params\": self.projection_head.parameters(), \n",
    "            \"lr\": BASE_LR, \n",
    "            \"weight_decay\": WEIGHT_DECAY\n",
    "        })\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(param_groups, eps=1e-6)\n",
    "\n",
    "        def lr_schedule_fn(current_step):\n",
    "            if current_step < WARMUP_EPOCHS:\n",
    "                return float(current_step) / float(max(1, WARMUP_EPOCHS))\n",
    "            else:\n",
    "                progress = float(current_step - WARMUP_EPOCHS) / float(max(1, EPOCHS - WARMUP_EPOCHS))\n",
    "                return max(0.0, 0.5 * (1.0 + torch.cos(torch.tensor(progress * 3.1415926535))))\n",
    "\n",
    "        scheduler = {\n",
    "            \"scheduler\": LambdaLR(optimizer, lr_lambda=lr_schedule_fn),\n",
    "            \"interval\": \"epoch\",\n",
    "            \"frequency\": 1,\n",
    "        }\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at /media/icnlab/Data/Manh/tinyML/checkpoints/eva02-15-1.94-0.00.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type            | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | backbone        | OptimizedModule | 58.1 M | train\n",
      "1 | projection_head | Sequential      | 1.1 M  | train\n",
      "2 | loss_fn         | SupConLoss      | 0      | train\n",
      "------------------------------------------------------------\n",
      "16.1 M    Trainable params\n",
      "43.2 M    Non-trainable params\n",
      "59.3 M    Total params\n",
      "237.034   Total estimated model params size (MB)\n",
      "581       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Restored all states from the checkpoint at /media/icnlab/Data/Manh/tinyML/checkpoints/eva02-15-1.94-0.00.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started at: 2025-05-29 21:36:21                                   \n",
      "Epoch 16: 100%|██████████| 111/111 [01:47<00:00,  1.04it/s, v_num=33, val_loss=2.470]Epoch 16 completed in: 0:01:47\n",
      "Average epoch time: 0:01:47\n",
      "Estimated time remaining: 2:28:09\n",
      "Epoch 16: 100%|██████████| 111/111 [01:47<00:00,  1.04it/s, v_num=33, val_loss=2.470]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 3668: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated time remaining: 2:28:09\n",
      "Epoch 17: 100%|██████████| 111/111 [00:10<00:00, 10.85it/s, v_num=33, val_loss=2.660]Epoch 17 completed in: 0:00:10\n",
      "Average epoch time: 0:00:58\n",
      "Estimated time remaining: 1:20:10\n",
      "Epoch 17: 100%|██████████| 111/111 [00:10<00:00, 10.85it/s, v_num=33, val_loss=2.660]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 3779: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated time remaining: 1:20:10\n",
      "Epoch 18: 100%|██████████| 111/111 [00:09<00:00, 11.18it/s, v_num=33, val_loss=2.590]Epoch 18 completed in: 0:00:09\n",
      "Average epoch time: 0:00:42\n",
      "Estimated time remaining: 0:57:16\n",
      "Epoch 18: 100%|██████████| 111/111 [00:09<00:00, 11.18it/s, v_num=33, val_loss=2.590]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 3890: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated time remaining: 0:57:16\n",
      "Epoch 19: 100%|██████████| 111/111 [00:10<00:00, 11.06it/s, v_num=33, val_loss=2.540]Epoch 19 completed in: 0:00:10\n",
      "Average epoch time: 0:00:34\n",
      "Estimated time remaining: 0:45:46\n",
      "Epoch 19: 100%|██████████| 111/111 [00:10<00:00, 11.05it/s, v_num=33, val_loss=2.540]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 4001: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated time remaining: 0:45:46\n",
      "Epoch 20: 100%|██████████| 111/111 [00:10<00:00, 10.84it/s, v_num=33, val_loss=2.520]Epoch 20 completed in: 0:00:10\n",
      "Average epoch time: 0:00:29\n",
      "Estimated time remaining: 0:38:51\n",
      "Epoch 20: 100%|██████████| 111/111 [00:10<00:00, 10.83it/s, v_num=33, val_loss=2.520]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 5 records. Best score: 1.938. Signaling Trainer to stop.\n",
      "Epoch 20, global step 4112: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in: 0:02:42\n",
      "Finished at: 2025-05-29 21:39:03\n",
      "Epoch 20: 100%|██████████| 111/111 [00:13<00:00,  8.41it/s, v_num=33, val_loss=2.520]\n",
      "Best model checkpoint: /media/icnlab/Data/Manh/tinyML/checkpoints/eva02-15-1.94-0.00.ckpt\n",
      "Best model score: 1.9378\n"
     ]
    }
   ],
   "source": [
    "# Train module\n",
    "# ----------------------------\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import os\n",
    "\n",
    "# Create checkpoints directory if it doesn't exist\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "data = Dataset(DATA_DIR, BATCH_SIZE)\n",
    "model = EVA02Lightning()\n",
    "\n",
    "# Define checkpoint callback\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath= checkpoint_dir,\n",
    "    filename=\"eva02-{epoch:02d}-{val_loss:.2f}-{val_acc:.2f}\",\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,  # Save the 1 best models\n",
    "    save_last=True,  # Additionally save the last model\n",
    "    verbose=True,\n",
    "    auto_insert_metric_name=False\n",
    ")\n",
    "# Add early stopping callback (optional)\n",
    "early_stop_callback = pl.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=10,  # Stop if no improvement for 5 epochs\n",
    "    mode=\"min\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Add timer callback to track training time\n",
    "class TimingCallback(pl.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.epoch_start_time = None\n",
    "        self.training_start_time = None\n",
    "        self.epoch_times = []\n",
    "    \n",
    "    def on_train_start(self, trainer, pl_module):\n",
    "        self.training_start_time = time.time()\n",
    "        print(f\"Training started at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        self.epoch_start_time = time.time()\n",
    "        if trainer.current_epoch > 0 and len(self.epoch_times) > 0:\n",
    "            avg_epoch_time = sum(self.epoch_times) / len(self.epoch_times)\n",
    "            remaining_epochs = trainer.max_epochs - trainer.current_epoch\n",
    "            est_remaining_time = avg_epoch_time * remaining_epochs\n",
    "            print(f\"Estimated time remaining: {timedelta(seconds=int(est_remaining_time))}\")\n",
    "    \n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_time = epoch_end_time - self.epoch_start_time\n",
    "        self.epoch_times.append(epoch_time)\n",
    "\n",
    "        print(f\"Epoch {trainer.current_epoch} completed in: {timedelta(seconds=int(epoch_time))}\")\n",
    "\n",
    "        if len(self.epoch_times) > 0:\n",
    "            avg_epoch_time = sum(self.epoch_times) / len(self.epoch_times)\n",
    "            remaining_epochs = trainer.max_epochs - trainer.current_epoch - 1\n",
    "            est_remaining_time = avg_epoch_time * remaining_epochs\n",
    "\n",
    "            print(f\"Average epoch time: {timedelta(seconds=int(avg_epoch_time))}\")\n",
    "            print(f\"Estimated time remaining: {timedelta(seconds=int(est_remaining_time))}\")\n",
    "    \n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        total_time = time.time() - self.training_start_time\n",
    "        print(f\"\\nTraining completed in: {timedelta(seconds=int(total_time))}\")\n",
    "        print(f\"Finished at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "\n",
    "# Create the timing callback\n",
    "timing_callback = TimingCallback()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    accumulate_grad_batches=ACCUM_GRAD_STEPS,\n",
    "    precision=\"16-mixed\",\n",
    "    gradient_clip_val=1.0,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=\"auto\",  # multi-GPU nếu có\n",
    "    # strategy=\"ddp_find_unused_parameters_false\", \n",
    "    log_every_n_steps=10,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback, timing_callback],  # Add callbacks here\n",
    ")\n",
    "\n",
    "# Run trainer\n",
    "trainer.fit(model, datamodule=data, \n",
    "ckpt_path='/media/icnlab/Data/Manh/tinyML/checkpoints/eva02-15-1.94-0.00.ckpt'\n",
    ")\n",
    "\n",
    "# Print path to best model checkpoint\n",
    "print(f\"Best model checkpoint: {checkpoint_callback.best_model_path}\")\n",
    "print(f\"Best model score: {checkpoint_callback.best_model_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "features = {}\n",
    "def hook_fn(module, input, output):\n",
    "    features['pre_head'] = output.detach() \n",
    "#\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "# best_model = EVA02Lightning.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "model = EVA02Lightning.load_from_checkpoint('/media/icnlab/Data/Manh/tinyML/checkpoints/eva02-15-1.94-0.00.ckpt')\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "model_name = 'eva02'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image('/media/icnlab/Data/Manh/tinyML/FieldPlant-11/cropped/Cassava Bacterial Blight/Manioc_Bacteriose-1-_jpg.rf.5d7291ff10dcb373dc5597797da735e8_3089_Cassava Bacterial Blight.jpg')\n",
    "image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    output = model(image_tensor)  # Nếu bạn đã định nghĩa forward phù hợp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_head_layer = model.projection_head\n",
    "Linear_layer = model.projection_head[2]\n",
    "Linear_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.projection_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "# read PDDD anotation\n",
    "%store -r image_dict\n",
    "def get_PDDD_info(image_id):\n",
    "    img_info = image_dict[image_id]\n",
    "    return img_info[\"file_name\"]\n",
    "\n",
    "%store -r category_dict\n",
    "def get_PDDD_category(category_id):\n",
    "    return category_dict[category_id]\n",
    "\n",
    "%store -r image_paths labels\n",
    "\n",
    "from collections import Counter\n",
    "labels_list = Counter(labels)\n",
    "\n",
    "image_paths[:2], labels[:2], len(image_paths), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common utils\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.416, 0.468, 0.355],   # normalize\n",
    "                         std=[0.210, 0.206, 0.213])\n",
    "])  # in PDDD paper\n",
    "\n",
    "###############################    \n",
    "def load_image(image_path):\n",
    "    if image_path.startswith('http'):\n",
    "        # Download image from URL\n",
    "        response = requests.get(image_path)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        # Load local image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new embedding utils\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# ===== INPUT =====\n",
    "# image_paths = List(...)\n",
    "# labels = List(...)\n",
    "\n",
    "def model_forward(model_name, tensor):\n",
    "    match model_name:\n",
    "        case '' :\n",
    "            embedding =  model.forward_features(tensor)\n",
    "\n",
    "        case 'eva02':\n",
    "            output = model(tensor)\n",
    "            embedding = output[1]\n",
    "            # embedding = features['pre_head']\n",
    "            # embedding = torch.nn.functional.normalize(embedding, dim=1)\n",
    "        \n",
    "    # print(embedding.shape)\n",
    "    return embedding\n",
    "        \n",
    "def create_embedding(model, image):\n",
    "    img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = model_forward(model_name,img_tensor)\n",
    "    return embedding.cpu().squeeze(0)\n",
    "\n",
    "def create_index(embeddings):\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "def run_model(model,image_paths, labels):\n",
    "    embeddings = []\n",
    "    for path, label in tqdm(list(zip(image_paths, labels)), desc=\"Extract embeddings\"):  # giới hạn 300 ảnh\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        embedding = create_embedding(model, img)\n",
    "        embeddings.append(embedding)\n",
    "        \n",
    "    labels = labels[:len(embeddings)]\n",
    "    embeddings_np = np.array(embeddings).astype(\"float32\")\n",
    "    # hook_handle.remove()  \n",
    "    return embeddings_np, labels\n",
    "\n",
    "embeddings_np, labels = run_model(model, image_paths, labels)\n",
    "index = create_index(embeddings_np)\n",
    "embedding_dim = embeddings_np.shape[1]\n",
    "print(embeddings_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy\n",
    "def inference(model,index,embedding, i):\n",
    "        query_vec = embedding.reshape(1, -1)\n",
    "        distance, result = index.search(query_vec, k=2)  # i: nearest vector, D: distance\n",
    "        top1 = result[0][1] if result[0][0] == i else result[0][0]\n",
    "        # top1 = result[0][0]\n",
    "        return top1, distance\n",
    "\n",
    "# Evaluate\n",
    "def evaluate(index, labels, embeddings):\n",
    "    correct = 0\n",
    "    total = len(labels)\n",
    "    pred_labels = []\n",
    "    distances = []\n",
    "    for i in tqdm(range(len(embeddings))):\n",
    "        top1, distance = inference(model,index,embeddings[i],i)\n",
    "        pred_labels.append(labels[top1])\n",
    "        distances.append(distance)\n",
    "        if labels[i] == labels[top1]:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / len(embeddings)\n",
    "    return accuracy, pred_labels, distances\n",
    "\n",
    "accuracy, pred_labels, distances = evaluate(index, labels, embeddings_np)\n",
    "accuracy, pred_labels[:3], distances[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "image = load_image('/media/icnlab/Data/Manh/tinyML/FieldPlant-11/cropped/Cassava Bacterial Blight/Manioc_Bacteriose-1-_jpg.rf.5d7291ff10dcb373dc5597797da735e8_3089_Cassava Bacterial Blight.jpg')\n",
    "embedding = create_embedding(model,image)\n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'FieldPlant-11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Giảm chiều bằng PCA hoặc t-SNE\n",
    "print(embeddings_np.shape)\n",
    "pca = PCA(n_components=3) # n_components <= min(n_samples, n_features)\n",
    "X_reduced = pca.fit_transform(embeddings_np) # n_features => 2 features,\n",
    "\n",
    "# Vẽ\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X_reduced[:, 0], y=X_reduced[:, 1], hue=labels, palette=\"tab10\", s=50, alpha=0.7)\n",
    "plt.title(f\"PCA embedding space {data_name} dataset by {model_name}\")\n",
    "plt.legend(loc='best', bbox_to_anchor=(1, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vẽ PCA 3D\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode string labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "label_ids = label_encoder.fit_transform(labels)  # array of ints\n",
    "\n",
    "# Dùng label_ids để vẽ màu\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2],\n",
    "    c=label_ids,\n",
    "    cmap='tab10',\n",
    "    s=50,\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Gắn tên class tương ứng với màu\n",
    "legend_labels = label_encoder.classes_\n",
    "legend_handles = [plt.Line2D([0], [0], marker='o', color='w',\n",
    "                             label=cls, markerfacecolor=plt.cm.tab10(i / len(legend_labels)),\n",
    "                             markersize=8)\n",
    "                  for i, cls in enumerate(legend_labels)]\n",
    "\n",
    "ax.set_title(f\"PCA 3D embedding of {data_name} dataset by {model_name}\")\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.set_zlabel(\"PC3\")\n",
    "ax.legend(handles=legend_handles, bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Class\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering metrics \n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, silhouette_score\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_clustering(embeddings, labels):\n",
    "    embeddings = np.array(embeddings)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Get number of unique classes for k-means\n",
    "    k = len(np.unique(labels))\n",
    "    \n",
    "    # Perform k-means clustering\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    pred_clusters = kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    metrics = {\n",
    "        'purity': cluster_purity(labels, pred_clusters),\n",
    "        'ari': adjusted_rand_score(labels, pred_clusters),\n",
    "        'nmi': normalized_mutual_info_score(labels, pred_clusters),\n",
    "        'silhouette': silhouette_score(embeddings, pred_clusters)\n",
    "    }\n",
    "    \n",
    "    return metrics, pred_clusters\n",
    "\n",
    "def cluster_purity(true_labels, cluster_labels):\n",
    "    cluster_purity_sum = 0\n",
    "    for cluster in np.unique(cluster_labels):\n",
    "        indices = np.where(cluster_labels == cluster)[0]\n",
    "        true_labels_in_cluster = true_labels[indices]\n",
    "        most_common_label, count = Counter(true_labels_in_cluster).most_common(1)[0]\n",
    "        cluster_purity_sum += count\n",
    "    return cluster_purity_sum / len(true_labels)\n",
    "\n",
    "# Evaluate clustering\n",
    "metrics, pred_clusters = evaluate_clustering(embeddings_np, labels)\n",
    "\n",
    "# Print results\n",
    "print(\"Clustering Evaluation Results:\")\n",
    "print(f\"Cluster Purity: {metrics['purity']:.4f}\")\n",
    "print(f\"Adjusted Rand Index (ARI): {metrics['ari']:.4f}\")\n",
    "print(f\"Normalized Mutual Information (NMI): {metrics['nmi']:.4f}\")\n",
    "print(f\"Silhouette Score: {metrics['silhouette']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
