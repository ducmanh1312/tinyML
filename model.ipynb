{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "features = {}\n",
    "def hook_fn(module, input, output):\n",
    "    features['pre_head'] = output.detach() \n",
    "#\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load eva02_large_patch14_clip_336 from timm\n",
    "import timm\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_path = \"hf_hub:timm/eva02_large_patch14_clip_336.merged2b_ft_inat21\"\n",
    "model = timm.create_model(model_path, pretrained=True)\n",
    "# model = torch.nn.Sequential(*list(model.children())[:-1]).to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "hook_handle = model.norm.register_forward_hook(hook_fn)\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ConvNeXt\n",
    "import torch\n",
    "from PDDD.Codes.modelpy.visual_model.ConvNeXt import convnext_base\n",
    "\n",
    "# # Khởi tạo kiến trúc mô hình trước\n",
    "model = convnext_base(6)\n",
    "\n",
    "# # Load trọng số đã lưu\n",
    "model_path = \"/media/icnlab/Data/Manh/tinyML/PDDD/model/ConvNeXt.std\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=device), strict=False)\n",
    "model=model.to(device)\n",
    "# model.eval()  # Đặt model ở chế độ inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ViT_L, ViLT\n",
    "import torch\n",
    "from PDDD.Codes.modelpy.visual_model.ViT_L import VisionTransformer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Khởi tạo kiến trúc mô hình trước\n",
    "model = VisionTransformer()\n",
    "\n",
    "# Load trọng số đã lưu\n",
    "model_path = \"/media/icnlab/Data/Manh/tinyML/PDDD/model/ViT_L.std\"\n",
    "# model_path = \"/media/icnlab/Data/Manh/tinyML/PDDD/model/ViLT.mdl\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=device), strict=False)\n",
    "model=model.to(device)\n",
    "\n",
    "# hook_handle = classification_head.head[1].register_forward_hook(hook_fn)\n",
    "ishook = True\n",
    "\n",
    "# model.eval()  # Đặt model ở chế độ inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icnlab/anaconda3/envs/aiot/lib/python3.11/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# load ResNet152\n",
    "import torch\n",
    "import os\n",
    "from PDDD.Codes.modelpy.visual_model.ResNet_50_101_152 import ResNet152\n",
    "\n",
    "model = ResNet152()\n",
    "\n",
    "# Load trọng số đã lưu\n",
    "model_path = \"/media/icnlab/Data/Manh/tinyML/PDDD/model/ResNet152.std\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model=model.to(device)\n",
    "\n",
    "# hook_handle = model.avgpool.register_forward_hook(hook_fn)\n",
    "model_name = os.path.basename(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MobileNetV3\n",
    "import torch\n",
    "import os\n",
    "from PDDD.Codes.modelpy.visual_model.MobileNetV3 import mobilenet_v3_small, mobilenet_v3_large\n",
    "\n",
    "model = mobilenet_v3_small()\n",
    "model_path = '/media/icnlab/Data/Manh/tinyML/PDDD/model/MobileNetV3.std'\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model=model.to(device)\n",
    "\n",
    "model_name = os.path.basename(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "backbone = nn.Sequential(*list(model.children()))  \n",
    "backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2048, out_features=1024, bias=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_head = dict(model.named_children())['fc']\n",
    "classification_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.layer4)\n",
    "print(model.fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng tensor cần training: 32\n",
      "Tổng số tham số cần training: 17062912\n",
      "Tổng số tham số       : 60,241,984\n",
      "Số tham số cần training: 17,062,912\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(('conv1', 'layer1', 'layer2', 'layer3')):\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "num_trainable_params = sum(1 for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Số lượng tensor cần training: {num_trainable_params}\")\n",
    "\n",
    "num_trainable_elements = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Tổng số tham số cần training: {num_trainable_elements}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Tổng số tham số       : {total_params:,}\")\n",
    "print(f\"Số tham số cần training: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "model = model.to(\"cuda\" or \"cpu\")\n",
    "summary(model, input_size=(3, 224, 224))  # Thay đổi input_size tùy vào model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['/media/icnlab/Data/Manh/tinyML/FieldPlant-11/cropped/Tomato blight leaf/IMG_20221118_082850_368_jpg.rf.61b5ec99d23a3f0c5a1766e235fb3b6b_1086_Tomato blight leaf.jpg',\n",
       "  '/media/icnlab/Data/Manh/tinyML/FieldPlant-11/cropped/Tomato blight leaf/IMG_20221118_081723_527_jpg.rf.337930f270075f587a13824248c1e5bd_3814_Tomato blight leaf.jpg'],\n",
       " ['Tomato blight leaf', 'Tomato blight leaf'],\n",
       " 2765,\n",
       " 2765)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read PDDD anotation\n",
    "%store -r image_dict\n",
    "def get_PDDD_info(image_id):\n",
    "    img_info = image_dict[image_id]\n",
    "    return img_info[\"file_name\"]\n",
    "\n",
    "%store -r category_dict\n",
    "def get_PDDD_category(category_id):\n",
    "    return category_dict[category_id]\n",
    "\n",
    "%store -r image_paths labels\n",
    "\n",
    "# from collections import Counter\n",
    "# list = Counter(labels)\n",
    "\n",
    "image_paths[:2], labels[:2], len(image_paths), len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common utils\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.416, 0.468, 0.355],   # normalize\n",
    "                         std=[0.210, 0.206, 0.213])\n",
    "])  # in PDDD paper\n",
    "\n",
    "###############################    \n",
    "def load_image(image_path):\n",
    "    if image_path.startswith('http'):\n",
    "        # Download image from URL\n",
    "        response = requests.get(image_path)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        # Load local image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained utils\n",
    "def predict_plant(image_path):\n",
    "    \"\"\"Predict plant species and return class ID and confidence\"\"\"\n",
    "    image = load_image(image_path)\n",
    "    model.eval()\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "        output = model(img_tensor)\n",
    "        probs = torch.nn.functional.softmax(output[0], dim=0)\n",
    "        confidence, pred_class = torch.topk(probs, 1)\n",
    "    return pred_class.item(), confidence.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run prediction\n",
    "pred_labels = []\n",
    "confidences = []\n",
    "for image_path in tqdm(image_paths[:10]):\n",
    "    pred_class_id, confidence = predict_plant(image_path)\n",
    "    # pred_label = get_PDDD_category(pred_class_id)\n",
    "    pred_labels.append(pred_class_id)\n",
    "    confidences.append(confidence)\n",
    "\n",
    "pred_labels[:10], confidences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extract embeddings: 100%|██████████| 2765/2765 [01:14<00:00, 37.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2765, 2048)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding utils\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def model_forward(model_name, tensor):\n",
    "    match model_name:\n",
    "        case 'ConvNeXt.std' | 'eva02_large_patch14_clip_336.merged2b_ft_inat21':\n",
    "            return model.forward_features(tensor)\n",
    "\n",
    "        case _:\n",
    "            output = model(tensor)\n",
    "            embedding = features['pre_head']\n",
    "            return embedding\n",
    "\n",
    "def get_image_embedding(model, image, device='cuda'):\n",
    "    img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = model_forward(model_name,img_tensor)  # lấy token embeddingput\n",
    "        # embedding = torch.nn.functional.normalize(embedding, dim=1)\n",
    "    return embedding.squeeze(0).cpu().numpy().reshape(1, -1)\n",
    "\n",
    "def create_index(embeddings):\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "def run_model(image_paths, labels):\n",
    "    embeddings = []\n",
    "    for path, label in tqdm(list(zip(image_paths, labels)), desc=\"Extract embeddings\"):  # giới hạn 300 ảnh\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        vec = get_image_embedding(model, img)\n",
    "        embeddings.append(vec)\n",
    "    labels = labels[:len(embeddings)]\n",
    "    # hook_handle.remove()  \n",
    "    return embeddings, labels\n",
    "\n",
    "# Evaluate\n",
    "def evaluate(index, labels, embeddings):\n",
    "    correct = 0\n",
    "    total = len(labels)\n",
    "    pred_labels = []\n",
    "    distances = []\n",
    "    for i in range(total):\n",
    "        top1, distance = inference(model,index,embeddings[i],i)\n",
    "        pred_labels.append(labels[top1])\n",
    "        distances.append(distance)\n",
    "        if labels[i] == labels[top1]:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy, pred_labels, distances\n",
    "\n",
    "embeddings,labels = run_model(image_paths, labels)\n",
    "embeddings_np = np.array(embeddings).astype(\"float32\").squeeze(1)  # FAISS cần float32\n",
    "index = create_index(embeddings_np)\n",
    "embeddings_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2765/2765 [00:02<00:00, 1003.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8491862567811935,\n",
       " ['Tomato Brown Spots', 'Tomato blight leaf', 'Tomato blight leaf'],\n",
       " [array([[  0.     , 129.12904]], dtype=float32),\n",
       "  array([[ 0.      , 52.070724]], dtype=float32)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate accuracy\n",
    "def inference(model,index,embedding, i):\n",
    "        query_vec = embedding.reshape(1, -1)\n",
    "        distance, result = index.search(query_vec, k=2)  # i: nearest vector, D: distance\n",
    "        top1 = result[0][1] if result[0][0] == i else result[0][0]\n",
    "        # top1 = result[0][0]\n",
    "        return top1, distance\n",
    "\n",
    "# Evaluate\n",
    "def evaluate(index, labels, embeddings):\n",
    "    correct = 0\n",
    "    total = len(labels)\n",
    "    pred_labels = []\n",
    "    distances = []\n",
    "    for i in tqdm(range(len(embeddings))):\n",
    "        top1, distance = inference(model,index,embeddings[i],i)\n",
    "        pred_labels.append(labels[top1])\n",
    "        distances.append(distance)\n",
    "        if labels[i] == labels[top1]:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / len(embeddings)\n",
    "    return accuracy, pred_labels, distances\n",
    "\n",
    "accuracy, pred_labels, distances = evaluate(index, labels, embeddings_np)\n",
    "accuracy, pred_labels[:3], distances[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image('/media/icnlab/Data/Manh/tinyML/FieldPlant-11/cropped/Charbon_de_mais-1-_jpg.rf.d83e40544e3be2dac4fd66077d50344a_7246_Corn Smut.jpg')\n",
    "embedding = get_image_embedding(model,image)\n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init path\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "dir_path = '/media/icnlab/Data/Manh/tinyML/FieldPlant-11'\n",
    "index_path = os.path.join(dir_path,f'{model_name}_trained.index')\n",
    "json_path = os.path.join(dir_path,f'{model_name}_trained.json')\n",
    "\n",
    "data_name = os.path.basename(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# save file index\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m faiss.write_index(\u001b[43mindex\u001b[49m, index_path)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# save class mapping (index -> class_id)\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(json_path, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[31mNameError\u001b[39m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "# save file index\n",
    "faiss.write_index(index, index_path)\n",
    "\n",
    "# save class mapping (index -> class_id)\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(labels, f)\n",
    "\n",
    "size_on_disk = os.path.getsize(index_path) / (1024 * 1024)\n",
    "print(f\"FAISS index file size: {size_on_disk:.2f} MB\")\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index file size: 10.88 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('ConvNeXt.std', 'FieldPlant-11', (2786, 1024), 2786)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read file index\n",
    "import faiss\n",
    "import json\n",
    "import os\n",
    "# Đọc index từ file\n",
    "index = faiss.read_index(index_path)\n",
    "embeddings_np = index.reconstruct_n(0, index.ntotal)\n",
    "\n",
    "# Đọc label mapping\n",
    "with open(json_path, \"r\") as f:\n",
    "    labels = json.load(f)  \n",
    "\n",
    "size_on_disk = os.path.getsize(index_path) / (1024 * 1024)\n",
    "print(f\"FAISS index file size: {size_on_disk:.2f} MB\")\n",
    "model_name,data_name, embeddings_np.shape, len(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
